# ğŸ§  LangGraph Multi-Agent Assistant

This project demonstrates a Multi-Agent Assistant framework built using **LangGraph**, **LangChain**, **RAG**, **VectorDB**, **Embedding**, **Agents**, and **Streamlit**. The assistant can answer complex user questions by breaking them into sub-questions, retrieving context-aware answers, and maintaining session memory â€” all while optionally using uploaded documents.

ğŸ“Œ **Key Features**

âœ… **Multi-Agent Collaboration** using LangGraph

âœ… **Planner Agent** breaks down a complex user query into smaller, focused sub-questions.

âœ… **Retriever Agent** answer each sub-question by searching an uploaded text file.

âœ… **Memory Node** to retain and display full conversation context

âœ… **Interactive UI** built with Streamlit (supports tabbed views)

âœ… **Optional Document Upload** to query custom knowledge

âœ… Powered by **OpenAI GPT-3.5 Turbo** and **FAISS vector store**

---

## âš¡ Project Structure

```
multi_agent_assistant/
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ planner_agent.py          # Planner: splits complex queries into sub-questions
â”‚   â””â”€â”€ retriever_agent.py        # Retriever: answers sub-questions using embeddings + FAISS
â”‚
â”œâ”€â”€ graph/
â”‚   â””â”€â”€ langgraph_multi_agent.py  # Graph wiring: planner â†’ retriever (loop) â†’ end.
|      # The controller node analyzes the query and determines whether to send it to the planner agent or the retrieve agent based on the complexity of the query. 
|        For example:
|        1. If the query is complex, the process follows this path: planner â†’ retriever (loop) â†’ end.
|        2. If the query is not complex, it directly navigates to the retriever (with no sub-questions).
|
â”‚
â”œâ”€â”€ about.txt                     # Knowledge base (This is the sample .txt file for testing. You can use your own .txt file and play with it.)
â”œâ”€â”€ app.py                        # Streamlit UI
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # Project documentation
```

---

## ğŸš€ Getting Started

### 1. Clone the repo

```bash
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>
```

### 2. Create and activate a virtual environment

```bash
python -m venv .venv
source .venv/bin/activate      # macOS/Linux
.venv\Scripts\activate         # Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up environment variables

Create a `.env` file in the project root:

```bash
OPENAI_API_KEY=your_openai_api_key_here
```

### 5. Run the Streamlit app

```bash
streamlit run app.py
```

---

## ğŸ§© How It Works

1. **User Input**:
   The user types a complex question into the Streamlit UI.

2. **Planner Agent** (`planner_agent.py`):
   Uses an LLM to generate a list of sub-questions.

3. **Retriever Agent** (`retriever_agent.py`):
   Uses embeddings + FAISS vector search to answer each sub-question using `about.txt`.

4. **LangGraph Orchestration** (`langgraph_multi_agent.py`):
   Wires planner â†’ retriever in a loop until all sub-questions are answered.

5. **UI Output** (`app.py`):
   Displays sub-questions and answers.

---

## ğŸ“š Example

**Input question:**

```
How do embeddings help improve RAG pipelines?
```

**Output (sample):**

* **Sub-questions**

  * What are embeddings?
  * What is RAG (Retrieval-Augmented Generation)?
  * How do embeddings support retrieval in RAG?

* **Answers**

  * *Q: What are embeddings?*
    *A: Embeddings are numerical representations of text or dataâ€¦*

---

## ğŸ› ï¸ Requirements

See [`requirements.txt`](requirements.txt) for exact package versions.
Core dependencies include:

* `streamlit` â€“ for the UI
* `langchain`, `langgraph` â€“ for workflow orchestration
* `langchain-openai`, `openai` â€“ for LLM access
* `faiss-cpu` â€“ for vector search
* `python-dotenv` â€“ for environment variable management

---

ğŸ“‚ Optional Enhancements (Coming Soon)

âœ… Add summarizer node (e.g., to condense large answers)

âœ… Add scoring/ranking agent

ğŸ”² Support for PDF/CSV uploads

ğŸ”² Add multi-modal support (text + image)

ğŸ”² Deploy to Streamlit Cloud

ğŸ¤– Future Work

 Actively working on:

ğŸ”² Fixing LangGraph update state errors

ğŸ”² Improving summarizer logic

ğŸ”² Enhancing memory with embeddings

ğŸ”² Making response flow more robust (parallel planning + retrieval)

---
