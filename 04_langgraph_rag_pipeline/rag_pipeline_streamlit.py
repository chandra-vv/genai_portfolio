# app.py

# âœ… Import required libraries
import os
import streamlit as st  # For building the UI
from PyPDF2 import PdfReader  # To read uploaded PDF files
from langchain_community.embeddings import OpenAIEmbeddings  # For converting text to embeddings
from langchain_openai import ChatOpenAI  # For LLM
from langchain_community.vectorstores import FAISS  # Vector store for similarity search
from langchain.text_splitter import CharacterTextSplitter  # To split PDF text
from dotenv import load_dotenv  # For loading OpenAI key from .env

# âœ… Load OpenAI API key from .env file
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# âœ… Initialize the embedding model
embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

# âœ… Streamlit UI
st.title("ðŸ“„ Ask Questions from your PDF (RAG-powered)")

# âœ… Upload PDF using Streamlit
pdf = st.file_uploader("Upload a PDF", type="pdf")

# âœ… Input box for questions
query = st.text_input("Ask a question about your PDF")

# âœ… Logic to process PDF and answer the query
if pdf and query:
    # âœ… Extract text from PDF
    pdf_reader = PdfReader(pdf)
    raw_text = ""
    for page in pdf_reader.pages:
        raw_text += page.extract_text()

    # âœ… Split text into small chunks
    text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_text(raw_text)

    # âœ… Create a FAISS vector store from the PDF text
    vectorstore = FAISS.from_texts(texts, embedding=embeddings)

    # âœ… Search for relevant chunks using the query
    docs = vectorstore.similarity_search(query)

    # âœ… Combine the most relevant chunks as context
    context = "\n\n".join([doc.page_content for doc in docs])

    # âœ… Create a prompt to send to LLM
    prompt = f"""Use the following context from the PDF to answer the question:

    {context}

    Question: {query}
    """

    # âœ… Initialize LLM (GPT-3.5)
    llm = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key)

    # âœ… Generate the answer
    response = llm.invoke(prompt)

    # âœ… Display the answer
    st.markdown("### ðŸ“¢ Answer:")
    st.write(response.content)
